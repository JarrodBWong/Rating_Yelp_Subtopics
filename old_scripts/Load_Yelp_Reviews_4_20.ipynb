{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import requests\n",
    "import codecs\n",
    "import urllib\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import BleiCorpus\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "def get_reviews(theurl, reviewCount):\n",
    "    reviewInfo = {}\n",
    "    \n",
    "    stop =  stop = set(stopwords.words('english'))\n",
    "    \n",
    "    main_page = urllib.request.urlopen(theurl)\n",
    "    soup = BeautifulSoup(main_page, \"html.parser\")\n",
    "    \n",
    "    review_div = soup.findAll('div',{'itemprop':'review'})\n",
    "\n",
    "#     reviewCount = 1\n",
    "    for i in review_div: # iterating through review_div \n",
    "        # get review star rating\n",
    "        reviewStar = float(i.find('meta',{'itemprop':'ratingValue'}).get('content', None))\n",
    "\n",
    "        # get review body text\n",
    "        reviewBody = i.find('p',{'itemprop':'description'})\n",
    "        for txt in reviewBody:\n",
    "            if type(txt) != '<p>' and not str(txt).startswith('<p>'):\n",
    "                reviewText = txt\n",
    "                \n",
    "                # Split review into sentences, remove stopwords, extract parts-of-speech tags\n",
    "                # (opt. if lots of reviews) store each review into MongoDB db called 'Reviews'\n",
    "\n",
    "                reviewWords = []\n",
    "                sentences = nltk.sent_tokenize(reviewText.lower())\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    tokens = nltk.word_tokenize(sentence)\n",
    "                    text = [w for w in tokens if w not in stop]\n",
    "                    tagged_text = nltk.pos_tag(text)\n",
    "                \n",
    "                for word, tag in tagged_text:\n",
    "                    reviewWords.append({'word': word, 'pos': tag})\n",
    "        \n",
    "        reviewInfo[reviewCount] = {'review_stars' : reviewStar, \\\n",
    "                                   'review_text' : reviewText, \\\n",
    "                                   'review_words' : reviewWords} \n",
    "        reviewCount += 1\n",
    "        \n",
    "    ## TODO: ITERATE THROUGH ALL PAGES OF REVIEWS FOR RESTAURANT ##\n",
    "        \n",
    "    return reviewInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(reviewDict):\n",
    "    # loop through the reviews\n",
    "    # get nouns and group them by lemma\n",
    "    reviewCorpus = {}\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    for review_count, review_content in reviewDict.items():\n",
    "        nouns = []\n",
    "        words = [w for w in review_content['review_words'] if w['pos'] in ['NN','NNS']]\n",
    "        \n",
    "        for w in words:\n",
    "            nouns.append(lemmatizer.lemmatize(w['word']))\n",
    "            \n",
    "        reviewCorpus[review_count] = {'review_stars' : review_content['review_stars'], \\\n",
    "                                      'review_text' : review_content['review_text'], \\\n",
    "                                      'review_nouns' : nouns} \n",
    "    \n",
    "    return reviewCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feed reviews to gensim LDA model\n",
    "\n",
    "def train(reviewDict):\n",
    "    \n",
    "    '''\n",
    "    create id2word\n",
    "    cannot filter extremes when the set is too small (only 1 page of yelp)\n",
    "    '''\n",
    "    id2word = corpora.Dictionary(reviewDict[review][\"review_nouns\"] for review in reviewDict)\n",
    "    #id2word.filter_extremes(keep_n=5)\n",
    "    #id2word.compactify()\n",
    "\n",
    "\n",
    "    corpora_dict = corpora.Dictionary(reviewDict[review][\"review_nouns\"] for review in reviewDict)\n",
    "    corpora_dict.save('lda/dictionary.dict')\n",
    "    \n",
    "    corpus = [corpora_dict.doc2bow(reviewDict[review][\"review_nouns\"]) for review in reviewDict]\n",
    "    corpora.BleiCorpus.serialize('lda/corpus.lda-c', corpus)\n",
    "    corpus = corpora.BleiCorpus('lda/corpus.lda-c')\n",
    "    \n",
    "    lda = gensim.models.LdaModel(corpus, num_topics=5, id2word=id2word)\n",
    "    lda.save('lda/lda_50_topics.lda')\n",
    "    \n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test: First page of Pavement reviews\n",
    "\n",
    "def main():\n",
    "    \n",
    "    '''\n",
    "    Loops though the pages of a restaurant and collects all the reviews\n",
    "    Probably split this up into another function later \n",
    "    \n",
    "    Needs to be extended to look at all restaurants over Boston\n",
    "    '''\n",
    "    \n",
    "    url = 'https://www.yelp.com/biz/pavement-coffeehouse-boston'\n",
    "    stop = 0\n",
    "    reviewDict ={}\n",
    "    reviewCount = 0\n",
    "    while(stop ==0):\n",
    "        reviewDict.update(get_reviews(url, reviewCount))\n",
    "\n",
    "        this_page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(this_page, \"html.parser\")\n",
    "        review_div = soup.findAll('link',{'rel':'next'})\n",
    "        if len(review_div) != 0:\n",
    "            for i in review_div:\n",
    "                url = i.get('href', None)\n",
    "\n",
    "        else:\n",
    "            stop = 1 \n",
    "        reviewCount += 20\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Check if the folder for the lda model exists\n",
    "    If it doesnt create the folder \n",
    "    '''\n",
    "    if not os.path.exists('lda'):\n",
    "        os.makedirs('lda')\n",
    "    \n",
    "#     print(train(lemmatize(reviewDict)))\n",
    "    train(lemmatize(reviewDict))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: (0, '0.037*\"time\" + 0.035*\"coffee\" + 0.022*\"service\" + 0.022*\"staff\" + 0.019*\"price\" + 0.015*\"shop\" + 0.015*\"drink\" + 0.015*\"friend\" + 0.014*\"sandwich\" + 0.012*\"pavement\"')\n",
      "\n",
      "#1: (1, '0.106*\"coffee\" + 0.042*\"place\" + 0.021*\"star\" + 0.016*\"service\" + 0.016*\"drink\" + 0.016*\"pavement\" + 0.014*\"area\" + 0.013*\"shop\" + 0.011*\"way\" + 0.011*\"ambiance\"')\n",
      "\n",
      "#2: (2, '0.076*\"place\" + 0.020*\"hour\" + 0.017*\"bagel\" + 0.016*\"location\" + 0.016*\"latte\" + 0.012*\"coffee\" + 0.012*\"service\" + 0.012*\"people\" + 0.012*\"music\" + 0.012*\"work\"')\n",
      "\n",
      "#3: (3, '0.033*\"bagel\" + 0.028*\"place\" + 0.027*\"pavement\" + 0.023*\"internet\" + 0.016*\"coffee\" + 0.016*\"time\" + 0.016*\"sandwich\" + 0.016*\"something\" + 0.012*\"day\" + 0.012*\"music\"')\n",
      "\n",
      "#4: (4, '0.020*\"hipster\" + 0.014*\"way\" + 0.014*\"hour\" + 0.014*\"staff\" + 0.014*\"wifi\" + 0.014*\"breakfast\" + 0.013*\"worth\" + 0.009*\"bagel\" + 0.008*\"place\" + 0.008*\"time\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_lda_topics():\n",
    "    dictionary_path = \"lda/dictionary.dict\"\n",
    "    corpus_path = \"lda/corpus.lda-c\"\n",
    "    lda_model_path = \"lda/lda_50_topics.lda\"\n",
    "\n",
    "    dictionary = corpora.Dictionary.load(dictionary_path)\n",
    "    corpus = corpora.BleiCorpus(corpus_path)\n",
    "    lda = LdaModel.load(lda_model_path)\n",
    "\n",
    "    i = 0\n",
    "    for topic in lda.show_topics(formatted=True):\n",
    "        print('#' + str(i) + ': ' + str(topic))\n",
    "        i += 1\n",
    "        print()\n",
    "        \n",
    "print_lda_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict topics for input review here "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
